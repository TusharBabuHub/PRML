{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e63cf4a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-24T17:00:09.956304Z",
     "iopub.status.busy": "2022-08-24T17:00:09.955519Z",
     "iopub.status.idle": "2022-08-24T17:00:09.968011Z",
     "shell.execute_reply": "2022-08-24T17:00:09.966967Z"
    },
    "papermill": {
     "duration": 0.0226,
     "end_time": "2022-08-24T17:00:09.970656",
     "exception": false,
     "start_time": "2022-08-24T17:00:09.948056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e788d7",
   "metadata": {
    "papermill": {
     "duration": 0.003572,
     "end_time": "2022-08-24T17:00:09.978566",
     "exception": false,
     "start_time": "2022-08-24T17:00:09.974994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To start with initialise the path of Images and Annotations and create a data frame of image data provided in xml kept in Annotations folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c0a075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T17:00:09.988551Z",
     "iopub.status.busy": "2022-08-24T17:00:09.987869Z",
     "iopub.status.idle": "2022-08-24T17:00:10.001948Z",
     "shell.execute_reply": "2022-08-24T17:00:10.001072Z"
    },
    "papermill": {
     "duration": 0.022385,
     "end_time": "2022-08-24T17:00:10.004799",
     "exception": false,
     "start_time": "2022-08-24T17:00:09.982414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "/kaggle/working/crop/\n",
      "/kaggle/input/road-sign-detection/\n",
      "/kaggle/input/road-sign-detection/images/\n",
      "/kaggle/input/road-sign-detection/annotations/\n"
     ]
    }
   ],
   "source": [
    "# Image paths\n",
    "img_base_out = os.getcwd()\n",
    "print(img_base_out)\n",
    "img_crop_path_out = img_base_out + '/crop/'\n",
    "print(img_crop_path_out)\n",
    "img_base_in = '/kaggle/input/road-sign-detection/'\n",
    "print(img_base_in)\n",
    "img_path_in = img_base_in + 'images/'\n",
    "print(img_path_in)\n",
    "img_ann_path_in = img_base_in + 'annotations/'\n",
    "print(img_ann_path_in)\n",
    "\n",
    "# Data Frame column header\n",
    "df_cols = ['ImgID', 'Width', 'Height', 'Type', 'xMin', 'yMin', 'xMax', 'yMax']\n",
    "\n",
    "# Initialise data frame for all files\n",
    "df_files = pd.DataFrame(columns=df_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547e87e",
   "metadata": {
    "papermill": {
     "duration": 0.003822,
     "end_time": "2022-08-24T17:00:10.012833",
     "exception": false,
     "start_time": "2022-08-24T17:00:10.009011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Get all the images and annotations in the directory\n",
    "Images are all in png format while annotations are xml format. xml files contain required metadata to read through the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1171b99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T17:00:10.023060Z",
     "iopub.status.busy": "2022-08-24T17:00:10.022626Z",
     "iopub.status.idle": "2022-08-24T17:00:10.241404Z",
     "shell.execute_reply": "2022-08-24T17:00:10.240366Z"
    },
    "papermill": {
     "duration": 0.227066,
     "end_time": "2022-08-24T17:00:10.244025",
     "exception": false,
     "start_time": "2022-08-24T17:00:10.016959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialise list of files\n",
    "list_of_files = []\n",
    "# Loop each xml file from annotation\n",
    "for dirname, _, filenames in os.walk(img_ann_path_in):\n",
    "    for file in filenames:\n",
    "        if (file.endswith(\".xml\")):\n",
    "            list_of_files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069694f0",
   "metadata": {
    "papermill": {
     "duration": 0.003748,
     "end_time": "2022-08-24T17:00:10.251953",
     "exception": false,
     "start_time": "2022-08-24T17:00:10.248205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The annotation files give details of different signals available in the images. It also provides us with the boundary of these signals. \n",
    "With this, we can extract multiple images of different signals from the data of boundary provided in the annotation file for each image. To create a predictive model, we would need to convert images into tabular data to assess and predict upon. Further sections of code would talk about analysis of data and the actions being taken via the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbafa410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T17:00:10.262052Z",
     "iopub.status.busy": "2022-08-24T17:00:10.261640Z",
     "iopub.status.idle": "2022-08-24T17:00:10.266451Z",
     "shell.execute_reply": "2022-08-24T17:00:10.265405Z"
    },
    "papermill": {
     "duration": 0.012689,
     "end_time": "2022-08-24T17:00:10.268867",
     "exception": false,
     "start_time": "2022-08-24T17:00:10.256178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialise the main image data array\n",
    "img_data = [(0) for _ in range(785)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0472e",
   "metadata": {
    "papermill": {
     "duration": 0.003711,
     "end_time": "2022-08-24T17:00:10.276725",
     "exception": false,
     "start_time": "2022-08-24T17:00:10.273014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Analysis\n",
    "* Images are all in png format\n",
    "* Each image is of different size\n",
    "* They may contain multiple signals of different sizes\n",
    "* Each image has a corressponding xml file which describes the different types of signal and their position in the image\n",
    "\n",
    "#### Further Actions\n",
    "* Extract the required data from xml file available for each image\n",
    "* Loop through each xml file and use the boundaries provided to crop the image and get just the signal image\n",
    "* Convert the image to a black and white image\n",
    "* Resize the images processed to 28 by 28 pixel dimension. This would create uniformity in the images and will help us in further conversion to an array\n",
    "* Save the processed images and segregate them based on signal type into different folders\n",
    "* Flatten the image data into array data so that we can use it for further prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cfe6fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T17:00:10.286576Z",
     "iopub.status.busy": "2022-08-24T17:00:10.286189Z",
     "iopub.status.idle": "2022-08-24T17:01:24.392794Z",
     "shell.execute_reply": "2022-08-24T17:01:24.391830Z"
    },
    "papermill": {
     "duration": 74.114826,
     "end_time": "2022-08-24T17:01:24.395646",
     "exception": false,
     "start_time": "2022-08-24T17:00:10.280820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et\n",
    "from PIL import Image\n",
    "\n",
    "# Create directory for saving cropped images\n",
    "if not os.path.exists(img_crop_path_out):\n",
    "    os.mkdir(img_crop_path_out, mode=0o777)\n",
    "\n",
    "# Loop through each image to get all signals present in it\n",
    "for file in list_of_files:\n",
    "    img_ann_file = img_ann_path_in + file\n",
    "    img_root = et.parse(img_ann_file)\n",
    "    filename = img_root.find('./filename').text\n",
    "    objects = img_root.findall('./object')\n",
    "    img_width = img_root.find('./size/width').text\n",
    "    img_height = img_root.find('./size/height').text\n",
    "\n",
    "    # create xml nested table\n",
    "    img_xml_data = [[filename, img_width, img_height, obj.find('./name').text, obj.find('bndbox/xmin').text, obj.find(\n",
    "        'bndbox/ymin').text, obj.find('bndbox/xmax').text, obj.find('bndbox/ymax').text] for obj in objects]\n",
    "\n",
    "    # create data frame out of the xml extracted data\n",
    "    df_xml = pd.DataFrame(img_xml_data, columns=df_cols)\n",
    "\n",
    "    # append the new file into the main data frame\n",
    "    df_files = pd.concat([df_files,df_xml], axis=0)\n",
    "\n",
    "    # open each image and read as balck and white\n",
    "    img = Image.open(img_path_in + filename).convert('L')\n",
    "\n",
    "    # set image count incase each image has multiple signals\n",
    "    img_count = 0\n",
    "\n",
    "    # loop through objects to get the boundary of signals in the image\n",
    "    for obj in objects:\n",
    "        xmin = int(obj.find('bndbox/xmin').text)\n",
    "        ymin = int(obj.find('bndbox/ymin').text)\n",
    "        xmax = int(obj.find('bndbox/xmax').text)\n",
    "        ymax = int(obj.find('bndbox/ymax').text)\n",
    "        img_type = obj.find('./name').text\n",
    "        img_count += 1\n",
    "\n",
    "        # crop image and resize them to dimesions of 28 by 28 pixels\n",
    "        img_crop = img.crop((xmin, ymin, xmax, ymax)).resize((28,28))\n",
    "\n",
    "        # make sub-directory for each signal type\n",
    "        if not os.path.exists(img_crop_path_out + img_type):\n",
    "            os.mkdir(img_crop_path_out + img_type, mode=0o777)\n",
    "\n",
    "        # save the cropped image in the specified signal type folder\n",
    "        crop_filepath = img_crop_path_out + img_type + '/' + str(img_count) + \"_\" + filename\n",
    "        img_crop.save(crop_filepath)\n",
    "        \n",
    "        # convert images into data\n",
    "        # a 28 by 28 array would be created\n",
    "        data = np.array(img_crop)\n",
    "        img_cnv = Image.fromarray(data.astype('uint8'))\n",
    "        img_cnv.save(crop_filepath)\n",
    "        \n",
    "        # move the 28 rows into single row, which would create 784 columns\n",
    "        data_flatten = data.reshape((1,784))\n",
    "        \n",
    "        # each would signify an image which was cropped and converted\n",
    "        flnm_array = np.array([str(img_count) + \"_\" + filename.rstrip('.png')])\n",
    "        \n",
    "        # add image name column to the array\n",
    "        data_flt = np.concatenate((flnm_array,data_flatten), axis = None)\n",
    "        \n",
    "        # include the new image data into array of previously saved images\n",
    "        img_data = np.vstack((img_data,data_flt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127b76a",
   "metadata": {
    "papermill": {
     "duration": 0.003854,
     "end_time": "2022-08-24T17:01:24.403981",
     "exception": false,
     "start_time": "2022-08-24T17:01:24.400127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Check\n",
    "View one of the cropped, resized and, discoloured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03478c12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T17:01:24.414087Z",
     "iopub.status.busy": "2022-08-24T17:01:24.413669Z",
     "iopub.status.idle": "2022-08-24T17:01:24.608234Z",
     "shell.execute_reply": "2022-08-24T17:01:24.607275Z"
    },
    "papermill": {
     "duration": 0.20243,
     "end_time": "2022-08-24T17:01:24.610479",
     "exception": false,
     "start_time": "2022-08-24T17:01:24.408049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f3c8f72d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,2))\n",
    "image = img_cnv\n",
    "plt.imshow(np.array(image).reshape(28,28), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519dd24",
   "metadata": {
    "papermill": {
     "duration": 0.0042,
     "end_time": "2022-08-24T17:01:24.619301",
     "exception": false,
     "start_time": "2022-08-24T17:01:24.615101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Check\n",
    "View the image array data created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f46d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T17:01:24.630566Z",
     "iopub.status.busy": "2022-08-24T17:01:24.629281Z",
     "iopub.status.idle": "2022-08-24T17:01:24.637503Z",
     "shell.execute_reply": "2022-08-24T17:01:24.636225Z"
    },
    "papermill": {
     "duration": 0.016215,
     "end_time": "2022-08-24T17:01:24.639875",
     "exception": false,
     "start_time": "2022-08-24T17:01:24.623660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '0', '0', ..., '0', '0', '0'],\n",
       "       ['1_road732', '181', '185', ..., '107', '119', '110'],\n",
       "       ['2_road732', '100', '115', ..., '84', '103', '93'],\n",
       "       ...,\n",
       "       ['1_road620', '105', '98', ..., '90', '88', '86'],\n",
       "       ['1_road701', '68', '67', ..., '63', '63', '63'],\n",
       "       ['2_road701', '68', '68', ..., '72', '72', '74']], dtype='<U21')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb4d7c",
   "metadata": {
    "papermill": {
     "duration": 0.004155,
     "end_time": "2022-08-24T17:01:24.648613",
     "exception": false,
     "start_time": "2022-08-24T17:01:24.644458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Analysis\n",
    "* Initially, we are trying to identify the speed limits provided on the images. \n",
    "* This would mean, no action being taken on images associated to types CrossWalk, Traffic Light and, Stop Signals.\n",
    "* It would be easy for us to segregate the image type as the earlier cropped, resized and discoloured images which were stored in separate folders.\n",
    "\n",
    "#### Further Action\n",
    "* We will predict the characters of the images by defining the speed limit numbers.\n",
    "* Impute a new column with the speed limit numbers found on the image. This column would be used as the Response Variable for our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f09298",
   "metadata": {
    "papermill": {
     "duration": 0.004025,
     "end_time": "2022-08-24T17:01:24.657148",
     "exception": false,
     "start_time": "2022-08-24T17:01:24.653123",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 84.565253,
   "end_time": "2022-08-24T17:01:25.283743",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-24T17:00:00.718490",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
